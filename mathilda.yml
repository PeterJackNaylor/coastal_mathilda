#training
# max_iters: 2000 #00 #180 #00
epochs: 100
normalise_targets: True
test_epochs: 1
# test_frequency: 500 # 1000
save_model: True
ignore_nan: True
train_fraction: 0.2
seed: 42
#Models
model:
  name: RFF_st #or RFF/RFF_st/SIREN/WIRES/MFN
  hidden_nlayers: 5 #5
  hidden_width: 256 #256
  scale: 0.1 #5 #for SIREN: 30
  scale_time: 1.5 #1.5 #3 #1.5 #or None
  skip: False
  ## RFF specific
  mapping_size: 256 #256
  activation: tanh
  modified_mlp: False #True # if you have a seperate encoder for the spatial and temporal inputs.
  linear: HE # RWF # HE # Glorot # RWF Glorot does not work with SIREN
  # RWF Specific
  mean: 1
  std: 0.1
  # WIRES Specific
  omega0: 10
  sigma0: 40
  trainable: True


# Lambda values in the loss
validation_loss: mse
losses:
  mse:
    report: True
    bs: 4096
    loss_balancing: True #wether to be included in loss balancing
    ignore_nan: True
  # pde:
  #   report: True
  #   log: True
  #   lambda: 1.e-1
  #   temporal_causality: True
  #   bs: 8192 # 8192 #16384 # 65536 #32768 # 8192 # √ # powers of 2 only
  #   method: pde
  #   loss_balancing: True #wether to be included in loss balancing
  #   penalty: L2
  # periodicity:
  #   report: False
  #   lambda: 1.e-1
  #   temporal_causality: True
  #   bs: 8192 # 8192 # 16384 # 32768 # powers of 2 only
  #   method: periodicity
  # gradient_lat:
  #   report: True
  #   log: True
  #   lambda: 1
  #   temporal_causality: False
  #   bs: 8192 # 8192 #16384 # 65536 #32768 # 8192 # √ # powers of 2 only
  #   method: gradient_lat
  #   loss_balancing: False #wether to be included in loss balancing
  # gradient_lon:
  #   report: True
  #   log: True
  #   lambda: 1
  #   temporal_causality: False
  #   bs: 8192 # 8192 #16384 # 65536 #32768 # 8192 # √ # powers of 2 only
  #   method: gradient_lon
  #   loss_balancing: False #wether to be included in loss balancing
  temporal_grad:
    report: True
    log: True
    lambda: 1
    temporal_causality: False
    bs: 8192 # 8192 #16384 # 65536 #32768 # 8192 # √ # powers of 2 only
    method: gradient_time
    loss_balancing: False #wether to be included in loss balancing





# learning schemes
temporal_causality:
  M: 32 # powers of 2 only
  eps: 8.e-3
  step: 1 #update


# Loss balancing
relobralo:
  status: False
  T: 1.
  alpha: 0.999
  rho: 0.5
  step: 100

self_adapting_loss_balancing:
  status: True
  alpha: 0.9
  step: 1000 #1000 #update


early_stopping:
  status: True
  ignore_first: 5
  patience: 20
  value: 0.0001

# optimizers
optimizer: AdamW
lr: 1.e-3
eps: 1.e-8 #adam precision
clip_gradients: True

learning_rate_decay:
  status: True
  # step: 2000
  epoch: 10
  gamma: 0.98 # 0.9

cosine_anealing:
  status: False
  min_eta: 0
  epoch: 2
  # step: 500

optuna:
  patience: 10000
  trials: 200
